{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TYPES OF TOKENIZATION\n",
    "+ Word Tokenization  \n",
    "\"Natural Language Processing using NLTK. Very interesting!\"  \n",
    "`['Natural', 'Language', 'Processing', 'using', 'NLTK', '.', 'Very', 'interesting', '!']`  \n",
    "+ Sentence Tokenization  \n",
    "\"Natural Language Processing using NLTK. Very interesting!\"  \n",
    "`['Natural Language Processing using NLTK.', 'Very interesting!']`  \n",
    "+ Subword Tokenization  \n",
    "\"tokenization\"  \n",
    "`[\"token\", \"ization\"]`  \n",
    "+ Character Tokenization  \n",
    "\"Tokenization\"  \n",
    "`[\"T\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\"]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Corpus  \n",
    "Body of a text.  \n",
    "+ Lexicon  \n",
    "Words and their meanings.  \n",
    "+ Token  \n",
    "Each “entity” that is a part of whatever was split up based on rules.  \n",
    "Each word is a token when a sentence is “tokenized” into words.   \n",
    "Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      " ['Natural Language Processing using NLTK.', 'Very interesting!']\n",
      " Word Tokenization:\n",
      " ['Natural', 'Language', 'Processing', 'using', 'NLTK', '.', 'Very', 'interesting', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing using NLTK. Very interesting!\"\n",
    "print(\"Sentence Tokenization:\\n\",sent_tokenize(text))\n",
    "print(\" Word Tokenization:\\n\",word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing using NLTK.', 'Very interesting!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "# Loading PunktSentenceTokenizer using English pickle file\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "\n",
    "sp_text = 'Hola amigo. Estoy bien.'\n",
    "\n",
    "spanish_tokenizer.tokenize(sp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour !', \"Comment ça va aujourd'hui ?\", \"J'espère que tout va bien.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "fr_text = \"Bonjour ! Comment ça va aujourd'hui ? J'espère que tout va bien.\"\n",
    "sentences = sent_tokenize(fr_text, language='french')\n",
    "\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'NLTK.',\n",
       " 'Very',\n",
       " 'interesting',\n",
       " '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WordPunctTokenizer\n",
    "Each punctuation is a token.  \n",
    "Eg \"Let's\" will be `'Let'. \"'\", 's'`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Let's see how it's working.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'see', 'how', 'it', 's', 'working']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"Let's see how it's working.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
