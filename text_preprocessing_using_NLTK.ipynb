{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Natural Language Toolkit NLTK library.  \n",
    "Text preprocessing is tha analysis of text data.  \n",
    "\n",
    "1. Import libraries  \n",
    "\n",
    "2. Text Lowercase    \n",
    "\n",
    "3. Remove numbers  \n",
    "\n",
    "4. Remove punctuation  \n",
    "\n",
    "5. Remove default stopwords  \n",
    "\n",
    "6. Stemming\n",
    "\n",
    "7. Lemmatization  \n",
    "\n",
    "8. Part of Speech Tagging\n",
    "\n",
    "9. Chunking\n",
    "\n",
    "10. Named Entity Recognition  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORT NECESSARY LIBRARIES\n",
    "nltk for text processing  \n",
    "string for string operations  \n",
    "re for regular expressions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "import string  # For string operations\n",
    "import re  # For regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TEXT LOWERCASE  \n",
    "Convert the whole text to lower case.  \n",
    "This creates consistency in the data.  \n",
    "It also simplifies tokenization and enhances model efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"oh my god she has 10 cubes left! i can't believe it. i thought that she had 15 ??\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "text_lowercase(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REMOVE NUMBERS\n",
    "+ Remove the numbers completely.  \n",
    "This can be done using regular expressions.  \n",
    "\n",
    "+ Convert the numbers to words.  \n",
    "Eg 3 to three.  \n",
    "This can be done using the inflect library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh my God she has  cubes left! I can't believe it. I thought that she had  ??\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)   #replace digits with empty string ''\n",
    "    return result\n",
    "\n",
    "input_str = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "remove_numbers(input_str)\n",
    "\n",
    "# \\d indicates digits 0-9\n",
    "# + indicates \"one or more occurrences\" eg '1', '3672'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh my God she has ten cubes left! I can't believe it. I thought that she had fifteen ??\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the inflect library\n",
    "import inflect\n",
    "\n",
    "#create engine object p, which provides access to the library's functions\n",
    "p = inflect.engine()\n",
    "\n",
    "def convert_number(text):\n",
    "    # split string into list of words/tokens\n",
    "    temp_str = text.split()\n",
    "    # initialise empty list\n",
    "    new_string = []\n",
    "\n",
    "    # loop through each word in temp_str\n",
    "    for word in temp_str:\n",
    "\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    # join all words of new_string to form one string separated by spaces\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "\n",
    "input_str = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "convert_number(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REMOVE PUNCTUATION\n",
    "This helps normalize the text.  \n",
    "Eg 'apple' and 'apple,' has different meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh my God she has 10 cubes left I cant believe it I thought that she had 15 '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    # str.maketrans() craetes a translation table\n",
    "    # Two empty spaces means we are not replacing anything\n",
    "    # string.punctuation provides list of characters to be removed\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "remove_punctuation(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are going tomorrow'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove whitespace from text\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "my_str = \"we are going  tomorrow\"\n",
    "remove_whitespace(my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REMOVE DEFAULT STOPWORDS\n",
    "stopwords from nltk.corpus contains ceratin common stopwords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))    #create set for english stop words\n",
    "    word_tokens = word_tokenize(text, language=\"english\")   #tokenize text to list of individual words\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words] #use list comprehension\n",
    "    return filtered_text\n",
    "\n",
    "input_string = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "remove_stopwords(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEMMING \n",
    "This is getting the **root** from a word.  \n",
    "We first convert the text into tokens, then convert tokens to their root form.  \n",
    "Porter, Snowball or Lancaster Stemmer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LEMMATIZATION  \n",
    "This is an NLP technique that reduces a word to its root form.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt') # Download the 'punkt' resource\n",
    "nltk.download('wordnet') # Download the 'wordnet' resource \n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return lemmas\n",
    "  \n",
    "input_str = \"Oh my God she has 10 cubes left! I can't believe it. I thought that she had 15 ??\"\n",
    "lemma_words(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PART OF SPEECH TAGGING\n",
    "This is necessary to understand the relationship between words.  \n",
    "It also helps in disambiguating words that have more than one meaning.  \n",
    "Eg book - verb and book - noun.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# convert text into word_tokens with their tags\n",
    "def pos_tagging(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging('You can do anything if you put your mind to it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tagset \n",
    "nltk.download('tagsets')\n",
    "\n",
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHUNKING  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
